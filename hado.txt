
HDFS, Sqoop and Hive

Storing, querying, indexing, transferring, streaming, and messaging

 
HDFS 			- Distributedistributed, portable filesystem, - High throughput streaming data access to applications that processes large datasets.
				  HDFS's main benefits are that it is fault-tolerant and designed to be run on commodity hardware
 
Mapreduce 		- Distributed, fault tolerant framework designed for processing large quantities of data stored in HDFS, in parallel on large clusters using commodity hardware
 
Apache Hive 	- Which is a data warehouse for managing large datasets stored in HDFS. Hive provides a HiveQL language
				  The data could be stored in one of the several supported formats, such as Avro , ORC, RCFile , Parquet , and SequenceFile.
			 	  Default is TextFile.
				
Apache HBase	- Hadoop database is called HBase. HBase is a distributed, scalable NoSQL data store providing real-time access to big data.

Apache Sqoop    - Apache Sqoop, a tool for bulk transfer of data between relational databases such as Oracle database and MySQL database and HDFS.
				  Sqoop also supports bulk transfer of data from RDBMS to Hive and HBase.
 
Apache flume    - Apache Flume, a distributed, fault-tolerant framework for collecting, aggregating, and streaming large datasets, 
				  Which are typically log data, but could be from other data sources such as relational databases.
 
Apache Avro  	- Apache Avro, a schema based data serialization system providing varied data structures. 
				  Avro provides a compact, fast, binary data format typically used to store persistent data.
				  
Apache Parquet	- Data format.
				  Parquet is a columnar data storage format providing complex data structures and efficient compression and encoding on columnar data to any Hadoop ecosystem project.
				  
Apache Kafka	- Distributed publish-subscribe messaging system.
				  Fast, scalable and durable
				  Kafka is designed to process large quantities of data and provide high throughput rates.

Apache Solr		- Distributed, scalable, and fault-tolerant framework.
				  Solr provides indexing and data integration for large datasets stored in HDFS, Hive, HBase, or relational databases. 
				  Solr is Lucene-based and one of the most commonly used search engines. 

Apache mahout	- Framework for machine learning applications.
				  Mahout supports several machine-learning systems, such as classification, clustering, and recommender systems.

				  MapReduce processes data stored in HDFS
				  HBase and Hive store data in HDFS by default
				  Sqoop could be used to bulk transfer data from a relational database management system (RDBMS) to HDFS, Hive, and HBase.
				  Sqoop also supports bulk transfer of data from HDFS to a relational database
				  Flume, which is based on sources and sinks, supports several kinds of sources and sinks with the emphasis being on streaming data in real time in contrast to one-time bulk transferring with Sqoop.
				  Flume is typically used to stream log data and the sink could be HDFS, Hive, HBase, or Solr.
				  Solr could be used to index data from HDFS, Hive, HBase, and RDBMS.
				  HDFS stores data in a disk filesystem and is in fact an abstract filesystem on top of the disk filesystem.
				  Solr also stores data in a disk filesystem by default, but can also store the indexed data in HDFS.
				  


Core Components of Apache Hadoop

Hadoop Distributed Filesystem (HDFS) and MapReduce


MapReduce processes input data as key/value pairs
MapReduce is designed to process medium-to-large files 
MapReduce has two phases: the map phase and the reduce phase.

The map phase uses one or more mappers to process the input data and the reduce phase uses zero or more reducers to process the data output during the map phase.

The input files is converted to key/value pairs before being input to the map phase. 
During the map phase, input data consisting of key/value pairs is mapped to output key/value pairs using a user-defined map() function .
 The map output data is partitioned and sorted for input to the reduce phase.
 The map output data is partitioned such that values associated with the same key are partitioned to the same partition and sent to the same reducer.
 During the reduce phase, the data output from the map phase is processed to reduce the number of values associated with a key, or using some other user-defined function.

 
Hadoop distributed filesystem

MapReduce framework

Setting the environment

Hadoop cluster modes

Running a MapReduce job with the MR1 framework

Running MR1 in standalone mode

Running MR1 in pseudo-distributed mode

Running MapReduce with the YARN framework

Running YARN in pseudo-distributed mode

Running Hadoop Streaming



Hadoop Distributed Filesystem
The Hadoop Distributed Filesystem (HDFS) is a distributed filesystem for storing data in the Hadoop ecosystem. 
HDFS stores data on commodity hardware and is designed to be fault tolerant, 
which implies that in a large cluster consisting of several (10s, 100s, or 1000s) nodes, 
some of the nodes are expected to fail periodically. 
Data is replicated over the cluster; three times by default in a fully-distributed filesystem. 
HDFS is different from ordinary distributed filesystems in that it has a much larger storage capacity and is designed to store large datasets. 
HDFS stores data in blocks, which is an abstraction over the underlying filesystem. 
The block abstraction is necessary as the size of a block is typically much larger than the size of a block on the underlying filesystem on a disk. 
An HDFS block is 128MB by default. HDFS consists of two components, called the NameNode and the DataNode. 
A fully-distributed Hadoop cluster has one NameNode and several DataNodes. The HDFS components are briefly discussed in the following sections.

NAMENODE
The NameNode is a master component and keeps the naming system of the files and directories managed by a Apache Hadoop cluster. 
HDFS stores data in blocks and NameNode keeps and manages the block locations of the files/directories.
NameNode tracks where data is kept, and when a client requests data, NameNode provides the block locations of the data requested. 
Similarly, when a client wants to store new data, the NameNode provides the block locations at which the data could be stored. 
NameNode does not store any data itself, nor does it directly access the DataNodes to read and write data.

DATANODES
The DataNodes store the data in Apache Hadoop. 
While the NameNode provides the clients with the block locations from which data can be read or at which new data can be stored, the DataNodes read and write requests from the clients. 
The DataNodes also perform the actual block creation, replication, and deletion duties under instruction from the the NameNode.

MapReduce Framework
As mentioned in the introduction, Apache Hadoop consists of two frameworks: HDFS and MapReduce.
 A Hadoop application is a MapReduce application consisting of two phases: the Map phase and the Reduce phase.
 Data in Hadoop is processed using key/value pairs. 
 During the Map phase, the input dataset is processed using the specified Map function.
 Data to the Map phase is typically input from HDFS. 
 The Map function could be designed to count the number of unique occurrences of a word in a dataset, as you’ll see in a subsequent example. 
 The Map phase generates an intermediate set of key/value pairs, which are sorted and partitioned and then transferred to the Reduce phase to be processed by a Reduce function.
 The output from the Reduce function is the output from a MapReduce application. The output from a MapReduce application is typically stored in the HDFS.

Two kinds of MapReduce frameworks are provided: MapReduce 1 (MR1) and YARN (MR2). 
The components in MR1 are JobTrackerand TaskTracker.
 A fully-distributed Hadoop cluster has one JobTracker and several TaskTrackers and the single JobTracker manages the several TaskTrackers.
 A MR1 Hadoop cluster is illustrated in Figure 2-2; the HDFS components are also shown.
 
JOBTRACKER
The JobTracker is a master component and it manages the jobs and resources in a Hadoop cluster. 
Clients submit MapReduce (MR) jobs to be run on Hadoop to the JobTracker and the JobTracker schedules and runs the jobs on TaskTrackers . 
As a MR job would typically process some data, the JobTracker gets the block locations for the data from the NameNode and schedules the jobs to run on TaskTrackers in proximity of the data. 
Because large quantities of data are involved, the computation to be performed on a dataset is moved to the data rather than shuffling the dataset to the computation as in a small-scale data processing framework.

TASKTRACKER
The TaskTracker s run the Map and Reduce tasks for a MapReduce application. A TaskTracker consists of “slots” in which a task runs. The “slots” are Map or Reduce task specific. A Map slot runs only a Map task and a Reduce slot runs only a Reduce task. The TaskTracker runs tasks under instruction from the JobTracker and, when a task completes, the TaskTracker notifies the JobTracker.



MR1 has the following drawbacks:

JobTracker manages and monitors both the resources and the MapReduce Job and task scheduling, 
which makes the JobTracker a single point of failure (SPOF) in a cluster. The cluster cannot be scaled efficiently.
Underutilization of resources as the slots to run a task are Map and Reduce task specific.
Non-MapReduce applications cannot be run concurrently.


To overcome these drawbacks, the MR2 (also called YARN) was designed. MR2 has the following advantages over MR1.

Better scalability as the JobTracker functionality is split between the ResourceManager and ApplicationMaster. Hadoop may be run on much larger clusters with MR2.
Better cluster utilization as the resource capacity configured for each node may be used by both Map and Reduce tasks.
Non-MapReduce clusters may be run on the same cluster concurrently.
Higher throughput uses finer-grained resource scheduling.




RESOURCEMANAGER
The ResourceManager is a master node and it accepts job submissions from clients and starts processes called ApplicationMaster to run the jobs. 
The ResourceManager also assigns resources required for a job. A job could be a MapReduce application or a non-MapReduce application. 
A ResourceManager consists of two components: Scheduler and ApplicationsManager. 
The Scheduler allocates resources and does not participate in running or monitoring an application. 
Resources are allocated as resource containers, with each resource container assigned a specific memory. 
The ApplicationsManager component accepts the job submissions from the clients and starts the ApplicationMaster to run the submitted applications. The ApplicationsManager also restarts failed ApplicationMaster. The ApplicationMasters are application specific, with one ApplicationMaster for each application. After the ApplicationsManager has obtained the resources for an ApplicationMaster to start an application, the ApplicationMaster obtains additional resources if required from the Scheduler component of the ResourceManager directly.

NODEMANAGER
The NodeManager is a per-machine component that runs resource containers on the machine and monitors the resource usage of the applications running in the resource containers on the machine. The NodeManager also reports the resource usage to the ResourceManager. When the ApplicationMasters start/stop or monitor the status of an application on a resource container, they do so via the NodeManager.

JOB HISTORYSERVER
Because MR2 does not include a JobTracker to keep information about a MapReduce job and the ApplicationMasters that actually run a MapReduce application terminate on completion, a Job HistoryServeris provided in MR2 to keep the information about the MapReduce jobs submitted (whether failed or successful) to a Hadoop cluster.


Setting the Environment

Apache Hadoop (version 2.6.0)
Java (version 7)

These examples use the Cloudera distribution of Apache Hadoop Cdh5.4.7 Hadoop 2.6.0. 
A later version may also be used. If you do use a different version, some of the configuration files and directory paths could be different, 
so be sure to modify the filenames and directory paths accordingly. Create a directory (/cdh for example) to install the software and set its permissions to global (777).



mkdir /cdh
chmod -R 777 /cdh
cd /cdh

Add a group called hadoop and add a user called hadoop to the hadoop group.

groupadd hadoop
useradd -g hadoop hadoop


tar zxvf jdk-7u55-linux-i586.gz


Download and extract the tar file for CDH 5.4.7 Hadoop 2.6.0.

wget http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.4.7.tar.gz
tar -xvf hadoop-2.6.0-cdh5.4.7.tar.gz
The CDH 5 packaging and built-in references make it necessary to create symlinks between certain directories. Create the following symlinks.

ln -s /cdh/hadoop-2.6.0-cdh5.4.7/bin /cdh/hadoop-2.6.0-cdh5.4.7/share/hadoop/mapreduce1/bin
ln -s /cdh/hadoop-2.6.0-cdh5.4.7/etc/hadoop  /cdh/hadoop-2.6.0-cdh5.4.7/share/hadoop/mapreduce1/c


A symlinkis just a pointer from one directory to another, implying that when a certain directory is referenced the pointed to, the directory is invoked. By creating the two symlinks, when the /cdh/hadoop-2.6.0-cdh5.4.7/share/hadoop/mapreduce1/bin is referenced or invoked, the /cdh/hadoop-2.6.0-cdh5.4.7/bin directory is referenced or invoked. Similarly, when the /cdh/hadoop-2.6.0-cdh5.4.7/share/hadoop/mapreduce1/conf directory is referenced, the /cdh/hadoop-2.6.0-cdh5.4.7/etc/hadoop directory is invoked.














				  
				  
Amazon EMR
				  
They must contain only lowercase letters, numbers, periods (.), and hyphens (-).
They cannot end in numbers.


Secure Shell protocol

https://console.aws.amazon.com/ec2/
https://console.aws.amazon.com/elasticmapreduce/

Amazon Elastic MapReduce (Amazon EMR) is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data


Create cluster

Clustername, (logging) , launch mode (cluster, Step execution), Software configuration ( Core hadoop, HBase, Presto, Spark)

Core Hadoop: Hadoop 2.8.4 with Ganglia 3.7.2, Hive 2.3.3, Hue 4.2.0, Mahout 0.13.0, Pig 0.17.0, and Tez 0.8.4
HBase: HBase 1.4.6 with Ganglia 3.7.2, Hadoop 2.8.4, Hive 2.3.3, Hue 4.2.0, Phoenix 4.14.0, and ZooKeeper 3.4.12
Presto: Presto 0.206 with Hadoop 2.8.4 HDFS and Hive 2.3.3 Metastore
Spark: Spark 2.3.1 on Hadoop 2.8.4 YARN with Ganglia 3.7.2 and Zeppelin 0.7.3



The HDFS daemons are the NameNode, Secondary NameNode, and DataNode.
The NameNode is the master daemon in the HDFS. NameNode’s function is to maintain the HDFS namespace metadata, which includes the filenames, 
directory names, file permissions, directory permissions, file-to-block mapping, block IDs, and block locations in RAM. 

The metadata is kept in RAM for fast access. NameNode stores the metadata information in a fsimage file in the NameNode’s local filesystem. 

The Secondary NameNode’s function is to make periodic checkpoints of the edits log to the fsimage file while the NameNode is running.

The DataNode is a slave daemon for storing the HDFS data. Data is broken into blocks and stored on the DataNodes. 


				  9884243134
				  

 
 
 
 